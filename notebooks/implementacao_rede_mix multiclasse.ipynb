{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csvs = \"D:/mestrado/notebooks/dados/features_csv\"\n",
    "cols_to_scale = ['RMSE', 'ZCR', 'COEF_1', 'COEF_2', 'COEF_3', 'COEF_4', 'COEF_5', 'COEF_6', 'COEF_7', 'COEF_8', 'COEF_9', 'COEF_10', 'COEF_11', 'COEF_12', 'COEF_13', 'COEF_14', 'COEF_15', 'COEF_16', 'COEF_17', 'COEF_18', 'COEF_19', 'COEF_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feminino_completo_normalizado = pd.read_csv(\"D:/mestrado/notebooks/dados/features_csv/feminino_completo_normalizado_balanceado.csv\", sep=\";\")\n",
    "\n",
    "df_masculino_completo_normalizado = pd.read_csv(\"D:/mestrado/notebooks/dados/features_csv/masculino_completo_normalizado_balanceado.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_masculino_completo_normalizado['Audio'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>COEF_1</th>\n",
       "      <th>COEF_2</th>\n",
       "      <th>COEF_3</th>\n",
       "      <th>COEF_4</th>\n",
       "      <th>COEF_5</th>\n",
       "      <th>COEF_6</th>\n",
       "      <th>COEF_7</th>\n",
       "      <th>COEF_8</th>\n",
       "      <th>...</th>\n",
       "      <th>COEF_14</th>\n",
       "      <th>COEF_15</th>\n",
       "      <th>COEF_16</th>\n",
       "      <th>COEF_17</th>\n",
       "      <th>COEF_18</th>\n",
       "      <th>COEF_19</th>\n",
       "      <th>COEF_20</th>\n",
       "      <th>Audio</th>\n",
       "      <th>Label Binario</th>\n",
       "      <th>Label Multiclasse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.515796</td>\n",
       "      <td>-0.935564</td>\n",
       "      <td>-0.007341</td>\n",
       "      <td>0.702159</td>\n",
       "      <td>0.167090</td>\n",
       "      <td>0.137820</td>\n",
       "      <td>0.091195</td>\n",
       "      <td>-0.127038</td>\n",
       "      <td>-0.060272</td>\n",
       "      <td>0.683259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488816</td>\n",
       "      <td>0.180111</td>\n",
       "      <td>-0.337470</td>\n",
       "      <td>-0.061166</td>\n",
       "      <td>0.039359</td>\n",
       "      <td>-0.470411</td>\n",
       "      <td>-0.180755</td>\n",
       "      <td>2583-disfonia_f_a_neutro</td>\n",
       "      <td>patologico</td>\n",
       "      <td>disfonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.372264</td>\n",
       "      <td>-0.900867</td>\n",
       "      <td>-0.022987</td>\n",
       "      <td>0.699735</td>\n",
       "      <td>0.175090</td>\n",
       "      <td>0.162464</td>\n",
       "      <td>0.048787</td>\n",
       "      <td>-0.067695</td>\n",
       "      <td>-0.057454</td>\n",
       "      <td>0.602664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442524</td>\n",
       "      <td>0.217840</td>\n",
       "      <td>-0.424841</td>\n",
       "      <td>0.022914</td>\n",
       "      <td>0.045451</td>\n",
       "      <td>-0.481561</td>\n",
       "      <td>-0.065944</td>\n",
       "      <td>2583-disfonia_f_a_neutro</td>\n",
       "      <td>patologico</td>\n",
       "      <td>disfonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.265795</td>\n",
       "      <td>-0.861214</td>\n",
       "      <td>-0.012809</td>\n",
       "      <td>0.689133</td>\n",
       "      <td>0.188743</td>\n",
       "      <td>0.166358</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>-0.117293</td>\n",
       "      <td>-0.051314</td>\n",
       "      <td>0.689141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525682</td>\n",
       "      <td>0.184777</td>\n",
       "      <td>-0.354476</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>-0.548674</td>\n",
       "      <td>-0.141514</td>\n",
       "      <td>2583-disfonia_f_a_neutro</td>\n",
       "      <td>patologico</td>\n",
       "      <td>disfonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.301426</td>\n",
       "      <td>-0.873606</td>\n",
       "      <td>-0.064827</td>\n",
       "      <td>0.673115</td>\n",
       "      <td>0.156481</td>\n",
       "      <td>0.241999</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>-0.155103</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.627511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569327</td>\n",
       "      <td>0.202097</td>\n",
       "      <td>-0.382755</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>0.070179</td>\n",
       "      <td>-0.437534</td>\n",
       "      <td>-0.261230</td>\n",
       "      <td>2583-disfonia_f_a_neutro</td>\n",
       "      <td>patologico</td>\n",
       "      <td>disfonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.318515</td>\n",
       "      <td>-0.881041</td>\n",
       "      <td>-0.068274</td>\n",
       "      <td>0.669738</td>\n",
       "      <td>0.229116</td>\n",
       "      <td>0.197704</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>-0.102621</td>\n",
       "      <td>0.025469</td>\n",
       "      <td>0.548966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609580</td>\n",
       "      <td>0.173431</td>\n",
       "      <td>-0.454408</td>\n",
       "      <td>0.058246</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>-0.418391</td>\n",
       "      <td>-0.318080</td>\n",
       "      <td>2583-disfonia_f_a_neutro</td>\n",
       "      <td>patologico</td>\n",
       "      <td>disfonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114491</th>\n",
       "      <td>-0.410155</td>\n",
       "      <td>-0.165354</td>\n",
       "      <td>0.088762</td>\n",
       "      <td>-0.235597</td>\n",
       "      <td>0.238339</td>\n",
       "      <td>-0.061283</td>\n",
       "      <td>-0.115726</td>\n",
       "      <td>0.119982</td>\n",
       "      <td>-0.686583</td>\n",
       "      <td>-0.101704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299613</td>\n",
       "      <td>-0.125993</td>\n",
       "      <td>0.050406</td>\n",
       "      <td>-0.014760</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.088621</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>1504-saudavel_f_a_neutro</td>\n",
       "      <td>saudavel</td>\n",
       "      <td>saudavel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114492</th>\n",
       "      <td>-0.423366</td>\n",
       "      <td>-0.165354</td>\n",
       "      <td>0.064783</td>\n",
       "      <td>-0.166293</td>\n",
       "      <td>0.218444</td>\n",
       "      <td>-0.122941</td>\n",
       "      <td>-0.080890</td>\n",
       "      <td>0.109747</td>\n",
       "      <td>-0.688766</td>\n",
       "      <td>-0.158311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372424</td>\n",
       "      <td>-0.188353</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>0.082224</td>\n",
       "      <td>0.033256</td>\n",
       "      <td>1504-saudavel_f_a_neutro</td>\n",
       "      <td>saudavel</td>\n",
       "      <td>saudavel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114493</th>\n",
       "      <td>-0.420284</td>\n",
       "      <td>-0.165354</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>-0.217303</td>\n",
       "      <td>0.281854</td>\n",
       "      <td>-0.095957</td>\n",
       "      <td>-0.095420</td>\n",
       "      <td>0.136499</td>\n",
       "      <td>-0.690650</td>\n",
       "      <td>-0.115877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336754</td>\n",
       "      <td>-0.149003</td>\n",
       "      <td>0.025027</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>0.082699</td>\n",
       "      <td>0.051755</td>\n",
       "      <td>0.053049</td>\n",
       "      <td>1504-saudavel_f_a_neutro</td>\n",
       "      <td>saudavel</td>\n",
       "      <td>saudavel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114494</th>\n",
       "      <td>-0.462307</td>\n",
       "      <td>-0.259843</td>\n",
       "      <td>0.075556</td>\n",
       "      <td>-0.205579</td>\n",
       "      <td>0.286340</td>\n",
       "      <td>-0.193938</td>\n",
       "      <td>-0.080814</td>\n",
       "      <td>0.109235</td>\n",
       "      <td>-0.685602</td>\n",
       "      <td>-0.253647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312777</td>\n",
       "      <td>-0.243346</td>\n",
       "      <td>0.069143</td>\n",
       "      <td>-0.060627</td>\n",
       "      <td>0.061201</td>\n",
       "      <td>0.128933</td>\n",
       "      <td>-0.065233</td>\n",
       "      <td>1504-saudavel_f_a_neutro</td>\n",
       "      <td>saudavel</td>\n",
       "      <td>saudavel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114495</th>\n",
       "      <td>-0.580843</td>\n",
       "      <td>-0.496063</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>-0.230645</td>\n",
       "      <td>0.247273</td>\n",
       "      <td>-0.138408</td>\n",
       "      <td>-0.114440</td>\n",
       "      <td>0.204627</td>\n",
       "      <td>-0.714394</td>\n",
       "      <td>-0.158915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265409</td>\n",
       "      <td>-0.097764</td>\n",
       "      <td>0.067935</td>\n",
       "      <td>-0.146569</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>0.166887</td>\n",
       "      <td>-0.064734</td>\n",
       "      <td>1504-saudavel_f_a_neutro</td>\n",
       "      <td>saudavel</td>\n",
       "      <td>saudavel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114496 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE       ZCR    COEF_1    COEF_2    COEF_3    COEF_4    COEF_5  \\\n",
       "0      -0.515796 -0.935564 -0.007341  0.702159  0.167090  0.137820  0.091195   \n",
       "1      -0.372264 -0.900867 -0.022987  0.699735  0.175090  0.162464  0.048787   \n",
       "2      -0.265795 -0.861214 -0.012809  0.689133  0.188743  0.166358  0.033080   \n",
       "3      -0.301426 -0.873606 -0.064827  0.673115  0.156481  0.241999  0.093699   \n",
       "4      -0.318515 -0.881041 -0.068274  0.669738  0.229116  0.197704  0.060397   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "114491 -0.410155 -0.165354  0.088762 -0.235597  0.238339 -0.061283 -0.115726   \n",
       "114492 -0.423366 -0.165354  0.064783 -0.166293  0.218444 -0.122941 -0.080890   \n",
       "114493 -0.420284 -0.165354  0.068558 -0.217303  0.281854 -0.095957 -0.095420   \n",
       "114494 -0.462307 -0.259843  0.075556 -0.205579  0.286340 -0.193938 -0.080814   \n",
       "114495 -0.580843 -0.496063  0.082237 -0.230645  0.247273 -0.138408 -0.114440   \n",
       "\n",
       "          COEF_6    COEF_7    COEF_8  ...   COEF_14   COEF_15   COEF_16  \\\n",
       "0      -0.127038 -0.060272  0.683259  ...  0.488816  0.180111 -0.337470   \n",
       "1      -0.067695 -0.057454  0.602664  ...  0.442524  0.217840 -0.424841   \n",
       "2      -0.117293 -0.051314  0.689141  ...  0.525682  0.184777 -0.354476   \n",
       "3      -0.155103 -0.001880  0.627511  ...  0.569327  0.202097 -0.382755   \n",
       "4      -0.102621  0.025469  0.548966  ...  0.609580  0.173431 -0.454408   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "114491  0.119982 -0.686583 -0.101704  ...  0.299613 -0.125993  0.050406   \n",
       "114492  0.109747 -0.688766 -0.158311  ...  0.372424 -0.188353  0.015625   \n",
       "114493  0.136499 -0.690650 -0.115877  ...  0.336754 -0.149003  0.025027   \n",
       "114494  0.109235 -0.685602 -0.253647  ...  0.312777 -0.243346  0.069143   \n",
       "114495  0.204627 -0.714394 -0.158915  ...  0.265409 -0.097764  0.067935   \n",
       "\n",
       "         COEF_17   COEF_18   COEF_19   COEF_20                     Audio  \\\n",
       "0      -0.061166  0.039359 -0.470411 -0.180755  2583-disfonia_f_a_neutro   \n",
       "1       0.022914  0.045451 -0.481561 -0.065944  2583-disfonia_f_a_neutro   \n",
       "2       0.067730  0.026291 -0.548674 -0.141514  2583-disfonia_f_a_neutro   \n",
       "3      -0.003269  0.070179 -0.437534 -0.261230  2583-disfonia_f_a_neutro   \n",
       "4       0.058246  0.070872 -0.418391 -0.318080  2583-disfonia_f_a_neutro   \n",
       "...          ...       ...       ...       ...                       ...   \n",
       "114491 -0.014760  0.084473  0.088621  0.049474  1504-saudavel_f_a_neutro   \n",
       "114492 -0.008895  0.048030  0.082224  0.033256  1504-saudavel_f_a_neutro   \n",
       "114493  0.040925  0.082699  0.051755  0.053049  1504-saudavel_f_a_neutro   \n",
       "114494 -0.060627  0.061201  0.128933 -0.065233  1504-saudavel_f_a_neutro   \n",
       "114495 -0.146569  0.178159  0.166887 -0.064734  1504-saudavel_f_a_neutro   \n",
       "\n",
       "        Label Binario  Label Multiclasse  \n",
       "0          patologico           disfonia  \n",
       "1          patologico           disfonia  \n",
       "2          patologico           disfonia  \n",
       "3          patologico           disfonia  \n",
       "4          patologico           disfonia  \n",
       "...               ...                ...  \n",
       "114491       saudavel           saudavel  \n",
       "114492       saudavel           saudavel  \n",
       "114493       saudavel           saudavel  \n",
       "114494       saudavel           saudavel  \n",
       "114495       saudavel           saudavel  \n",
       "\n",
       "[114496 rows x 25 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_masculino_completo_normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_paralisia_feminino = df_feminino_completo_normalizado[df_feminino_completo_normalizado['Label Multiclasse'] == 'paralisia']\n",
    "audios_unicos_femininos_palisia = df_paralisia_feminino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_feminino_paralisia = df_paralisia_feminino[df_paralisia_feminino['Audio'].isin(audios_unicos_femininos_palisia)]\n",
    "\n",
    "subset_df_feminino_paralisia['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_paralisia_masculino = df_masculino_completo_normalizado[df_masculino_completo_normalizado['Label Multiclasse'] == 'paralisia']\n",
    "audios_unicos_masculino_palisia = df_paralisia_masculino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_masculino_paralisia = df_paralisia_masculino[df_paralisia_masculino['Audio'].isin(audios_unicos_masculino_palisia)]\n",
    "\n",
    "subset_df_masculino_paralisia['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_polipo_feminino = df_feminino_completo_normalizado[df_feminino_completo_normalizado['Label Multiclasse'] == 'polipo']\n",
    "audios_unicos_femininos_palisia = df_polipo_feminino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_feminino_polipo = df_polipo_feminino[df_polipo_feminino['Audio'].isin(audios_unicos_femininos_palisia)]\n",
    "\n",
    "subset_df_feminino_polipo['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_polipo_masculino = df_masculino_completo_normalizado[df_masculino_completo_normalizado['Label Multiclasse'] == 'polipo']\n",
    "audios_unicos_masculino_palisia = df_polipo_masculino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_masculino_polipo = df_polipo_masculino[df_polipo_masculino['Audio'].isin(audios_unicos_masculino_palisia)]\n",
    "\n",
    "subset_df_masculino_polipo['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_laringite_feminino = df_feminino_completo_normalizado[df_feminino_completo_normalizado['Label Multiclasse'] == 'laringite']\n",
    "audios_unicos_femininos_palisia = df_laringite_feminino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_feminino_laringite = df_laringite_feminino[df_laringite_feminino['Audio'].isin(audios_unicos_femininos_palisia)]\n",
    "\n",
    "subset_df_feminino_laringite['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_laringite_masculino = df_masculino_completo_normalizado[df_masculino_completo_normalizado['Label Multiclasse'] == 'laringite']\n",
    "audios_unicos_masculinos_palisia = df_laringite_masculino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_masculino_laringite = df_laringite_masculino[df_laringite_masculino['Audio'].isin(audios_unicos_masculinos_palisia)]\n",
    "\n",
    "subset_df_masculino_laringite['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_disfonia_feminino = df_feminino_completo_normalizado[df_feminino_completo_normalizado['Label Multiclasse'] == 'disfonia']\n",
    "audios_unicos_femininos_palisia = df_disfonia_feminino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_feminino_disfonia = df_disfonia_feminino[df_disfonia_feminino['Audio'].isin(audios_unicos_femininos_palisia)]\n",
    "\n",
    "subset_df_feminino_disfonia['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_disfonia_masculino = df_masculino_completo_normalizado[df_masculino_completo_normalizado['Label Multiclasse'] == 'disfonia']\n",
    "audios_unicos_masculinos_palisia = df_disfonia_masculino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_masculino_disfonia = df_disfonia_masculino[df_disfonia_masculino['Audio'].isin(audios_unicos_masculinos_palisia)]\n",
    "\n",
    "subset_df_masculino_disfonia['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_saudavel_feminino = df_feminino_completo_normalizado[df_feminino_completo_normalizado['Label Multiclasse'] == 'saudavel']\n",
    "audios_unicos_femininos_palisia = df_saudavel_feminino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_feminino_saudavel = df_saudavel_feminino[df_saudavel_feminino['Audio'].isin(audios_unicos_femininos_palisia)]\n",
    "\n",
    "subset_df_feminino_saudavel['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 1: Selecionar 1000 áudios distintos aleatoriamente\n",
    "df_saudavel_masculino = df_masculino_completo_normalizado[df_masculino_completo_normalizado['Label Multiclasse'] == 'saudavel']\n",
    "audios_unicos_masculinos_palisia = df_saudavel_masculino['Audio'].drop_duplicates().sample(125, random_state=42)\n",
    "\n",
    "# Passo 2: Trazer todos os frames desses áudios\n",
    "subset_df_masculino_saudavel = df_saudavel_masculino[df_saudavel_masculino['Audio'].isin(audios_unicos_masculinos_palisia)]\n",
    "\n",
    "subset_df_masculino_saudavel['Audio'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix_completo_normalizado = pd.concat([subset_df_feminino_disfonia, \n",
    "                                         subset_df_feminino_paralisia,\n",
    "                                         subset_df_feminino_laringite,\n",
    "                                         subset_df_feminino_polipo,\n",
    "                                         subset_df_feminino_saudavel,\n",
    "                                        subset_df_masculino_disfonia,\n",
    "                                        subset_df_masculino_paralisia,\n",
    "                                        subset_df_masculino_laringite,\n",
    "                                        subset_df_masculino_polipo,\n",
    "                                        subset_df_masculino_saudavel],\n",
    "                                        ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_tensor(df, num_features):\n",
    "    grouped = df.groupby(\"Audio\")\n",
    "    # Cria uma lista de arrays, um para cada áudio\n",
    "    tensors = [group.iloc[:, :num_features].values for _, group in grouped]\n",
    "    return np.array(tensors)  # Converte para um tensor 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para padronizar as sequências (com padding ou truncamento)\n",
    "def standardize_frames(sequences, max_frames):\n",
    "    standardized_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_frames:\n",
    "            # Padding: Adiciona zeros ao final da sequência\n",
    "            padded_seq = np.pad(seq, ((0, max_frames - len(seq)), (0, 0)), mode='constant')\n",
    "            standardized_sequences.append(padded_seq)\n",
    "        else:\n",
    "            # Truncamento: Corta a sequência para o tamanho máximo\n",
    "            truncated_seq = seq[:max_frames, :]\n",
    "            standardized_sequences.append(truncated_seq)\n",
    "    return np.array(standardized_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_mix_completo_normalizado.groupby(\"Audio\")\n",
    "df_frames = pd.DataFrame()\n",
    "df_frames['frames'] = df_mix_completo_normalizado.groupby(\"Audio\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem  1250 áudios.\n",
      "Relação de áudios por classe:\n",
      "Label Multiclasse\n",
      "disfonia     250\n",
      "paralisia    250\n",
      "laringite    250\n",
      "polipo       250\n",
      "saudavel     250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Verifica a quantidade de audios por classe\n",
    "\n",
    "contagem = df_mix_completo_normalizado['Audio'].nunique()\n",
    "print(\"Existem \", contagem, \"áudios.\")\n",
    "\n",
    "df_distintos = df_mix_completo_normalizado.drop_duplicates(subset='Audio')\n",
    "contagem_categoria = df_distintos['Label Multiclasse'].value_counts()\n",
    "print(\"Relação de áudios por classe:\")\n",
    "print(contagem_categoria)\n",
    "\n",
    "assert len(set(contagem_categoria)) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119186\n",
      "numero_max_frames  95\n"
     ]
    }
   ],
   "source": [
    "df_frames.head()\n",
    "print(df_frames['frames'].sum())\n",
    "numero_max_frames = int(df_frames['frames'].mean())\n",
    "print('numero_max_frames ', numero_max_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_y(df):\n",
    "    num_features = 23  # RMS (1) + ZCR (1) + 20 MFCCs\n",
    "    grouped = df.groupby(\"Audio\")\n",
    "\n",
    "    # Converta cada áudio para uma matriz (frames x features)\n",
    "    sequences = [group.iloc[:, :num_features].values for _, group in grouped]\n",
    "\n",
    "    # Padronize os frames\n",
    "    max_frames = numero_max_frames  # Defina o número fixo de frames\n",
    "    tensor = standardize_frames(sequences, max_frames)\n",
    "\n",
    "    print(\"Shape do tensor:\", tensor.shape)  # Deve ser (num_audios, max_frames, num_features)\n",
    "\n",
    "    y = []\n",
    "    for s in sequences:\n",
    "        for element in s[0]:\n",
    "            if isinstance(element, str): \n",
    "                if 'saudavel' in element:\n",
    "                        y.append(0)\n",
    "                elif 'disfonia' in element:\n",
    "                     y.append(1)\n",
    "                elif 'laringite' in element:\n",
    "                     y.append(2)\n",
    "                elif 'paralisia' in element:\n",
    "                     y.append(3)\n",
    "                elif 'polipo' in element:\n",
    "                     y.append(4)\n",
    "                else:\n",
    "                    y.append(None)\n",
    "                break\n",
    "            \n",
    "    print('Len(y)')\n",
    "    print(len(y))\n",
    "\n",
    "    none_indexes = [i for i, val in enumerate(y) if val == None]\n",
    "    # print result\n",
    "    print(\"The None indices list is : \" + str(none_indexes))\n",
    "    assert len(none_indexes) == 0\n",
    "\n",
    "    # Transformação para one-hot\n",
    "    one_hot_labels = tf.keras.utils.to_categorical(y, num_classes=5)\n",
    "    print(one_hot_labels)\n",
    "\n",
    "    return one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensor_x(df):\n",
    "    num_features = 22  # RMS (1) + ZCR (1) + 20 MFCCs\n",
    "    # Agrupe o DataFrame por áudio\n",
    "    grouped = df.groupby(\"Audio\")\n",
    "\n",
    "    # Converta cada áudio para uma matriz (frames x features)\n",
    "    sequences = [group.iloc[:, :num_features].values for _, group in grouped]\n",
    "\n",
    "    # Padronize os frames\n",
    "    max_frames = numero_max_frames  # Defina o número fixo de frames\n",
    "    tensor = standardize_frames(sequences, max_frames)\n",
    "\n",
    "    print(\"Shape do tensor:\", tensor.shape)  # Deve ser (num_audios, max_frames, num_features)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a Rede Neural"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_grid_search(learning_rate=0.001, dropout_rate=0.2, lstm_units=128):\n",
    "\n",
    "\n",
    "    print(\"Iniciando o modelo com os seguintes parâmetros: Learning Rate - \", learning_rate, \" Dropout - \", dropout_rate, \" LSTM Units - \", lstm_units)\n",
    "\n",
    "    # Dimensão das features (exemplo: 20 MFCCs + RMS + ZCR -> 22 features por timestep)\n",
    "    input_shape = (None, 22)  # None -> Sequência variável, 22 -> Features por timestep\n",
    "\n",
    "    # Regularização L2 (lambda = 0.01)\n",
    "    l2_reg = regularizers.L2(0.005)\n",
    "\n",
    "    # Entrada\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # 5 Blocos CNN\n",
    "    x = inputs\n",
    "    for _ in range(6):\n",
    "        x = layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)  # Dropout adicional\n",
    "        # print(\"Após bloco CNN:\", x.shape)\n",
    "\n",
    "    # print(x.shape)\n",
    "\n",
    "    # LSTM com regularização L2\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2_reg)(x)\n",
    "    # print(\"Após LSTM (return_sequences=True):\", x.shape)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    # attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)  # 4 cabeças de atenção\n",
    "    # x = layers.Add()([x, attention])  # Resíduo para estabilizar treinamento\n",
    "    #print(\"Após Multi-Head Attention:\", x.shape)\n",
    "\n",
    "    # Atenção Personalizada\n",
    "    query = layers.Dense(256, activation=\"tanh\")(x)  # Projeta a sequência\n",
    "    key = layers.Dense(256, activation=\"tanh\")(x)    # Projeta a sequência\n",
    "    score = layers.Dot(axes=[2, 2])([query, key])    # Produto escalar\n",
    "    attention_weights = layers.Softmax()(score)     # Pesos normalizados\n",
    "    context = layers.Dot(axes=[2, 1])([attention_weights, x])  # Combina pesos com sequência\n",
    "\n",
    "    # Skip connection\n",
    "    x = layers.Add()([x, context])\n",
    "\n",
    "\n",
    "    # LSTM final para resumir a sequência\n",
    "    x = layers.LSTM(lstm_units, kernel_regularizer=l2_reg)(x)\n",
    "    # print(\"Após LSTM final:\", x.shape)\n",
    "\n",
    "    # Camadas densas para classificação com regularização e Dropout\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # print(\"Após Dense:\", x.shape)\n",
    "\n",
    "    # Camada de saída multiclasse\n",
    "    num_classes = 5\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2_reg)(x)\n",
    "    # print(\"Saída:\", outputs.shape)\n",
    "\n",
    "    # Construção do modelo\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    # Compilação\n",
    "    #optimizer = Adam(learning_rate=1e-3)  # Taxa de aprendizado inicial\n",
    "    # Learning Rate default do Adam = 0.001\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"categorical_accuracy\",  \"precision\", \"recall\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do tensor: (1250, 95, 22)\n",
      "Shape do tensor: (1250, 95, 23)\n",
      "Len(y)\n",
      "1250\n",
      "The None indices list is : []\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# dividindo tensor entre treino, validação e teste\n",
    "X = extract_tensor_x(df_mix_completo_normalizado)\n",
    "y = extract_y(df_mix_completo_normalizado)\n",
    "\n",
    "assert len(y) == 1250\n",
    "assert X.shape[0] == len(y)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2631 - categorical_accuracy: 0.2631 - loss: 7.2639 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6575 - categorical_accuracy: 0.6575 - loss: 5.4761 - precision: 0.2689 - recall: 0.0058      \n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7350 - categorical_accuracy: 0.7350 - loss: 3.9114 - precision: 0.8657 - recall: 0.3522\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8282 - categorical_accuracy: 0.8282 - loss: 3.0939 - precision: 0.8646 - recall: 0.7183\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8586 - categorical_accuracy: 0.8586 - loss: 2.6692 - precision: 0.8874 - recall: 0.8246\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8762 - categorical_accuracy: 0.8762 - loss: 2.4251 - precision: 0.8902 - recall: 0.8518\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8811 - categorical_accuracy: 0.8811 - loss: 2.1828 - precision: 0.8927 - recall: 0.8575\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9447 - categorical_accuracy: 0.9447 - loss: 1.9173 - precision: 0.9541 - recall: 0.9354\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9204 - categorical_accuracy: 0.9204 - loss: 1.8369 - precision: 0.9296 - recall: 0.9166\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9216 - categorical_accuracy: 0.9216 - loss: 1.7000 - precision: 0.9323 - recall: 0.9118\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9453 - categorical_accuracy: 0.9453 - loss: 1.6032 - precision: 0.9489 - recall: 0.9350\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9412 - categorical_accuracy: 0.9412 - loss: 1.4967 - precision: 0.9546 - recall: 0.9331\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9448 - categorical_accuracy: 0.9448 - loss: 1.4267 - precision: 0.9558 - recall: 0.9316\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9078 - categorical_accuracy: 0.9078 - loss: 1.4931 - precision: 0.9171 - recall: 0.8815\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9096 - categorical_accuracy: 0.9096 - loss: 1.4175 - precision: 0.9346 - recall: 0.9033\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9570 - categorical_accuracy: 0.9570 - loss: 1.2623 - precision: 0.9673 - recall: 0.9436\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9330 - categorical_accuracy: 0.9330 - loss: 1.3183 - precision: 0.9400 - recall: 0.9142\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8750 - categorical_accuracy: 0.8750 - loss: 1.4139 - precision: 0.9013 - recall: 0.8387\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9433 - categorical_accuracy: 0.9433 - loss: 1.2183 - precision: 0.9531 - recall: 0.9330\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9460 - categorical_accuracy: 0.9460 - loss: 1.1577 - precision: 0.9607 - recall: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.3345 - categorical_accuracy: 0.3345 - loss: 7.2846 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6521 - categorical_accuracy: 0.6521 - loss: 5.5137 - precision: 0.3839 - recall: 0.0168     \n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6994 - categorical_accuracy: 0.6994 - loss: 4.1326 - precision: 0.8277 - recall: 0.3764\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8248 - categorical_accuracy: 0.8248 - loss: 3.3307 - precision: 0.8723 - recall: 0.7310\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7985 - categorical_accuracy: 0.7985 - loss: 3.0085 - precision: 0.8259 - recall: 0.7653\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8503 - categorical_accuracy: 0.8503 - loss: 2.6905 - precision: 0.8846 - recall: 0.8171\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8991 - categorical_accuracy: 0.8991 - loss: 2.4131 - precision: 0.9215 - recall: 0.8696\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9477 - categorical_accuracy: 0.9477 - loss: 2.1694 - precision: 0.9499 - recall: 0.9262\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9197 - categorical_accuracy: 0.9197 - loss: 2.0648 - precision: 0.9270 - recall: 0.9100\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9093 - categorical_accuracy: 0.9093 - loss: 1.9736 - precision: 0.9156 - recall: 0.8982\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9151 - categorical_accuracy: 0.9151 - loss: 1.8837 - precision: 0.9270 - recall: 0.9092\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9234 - categorical_accuracy: 0.9234 - loss: 1.7806 - precision: 0.9322 - recall: 0.9117\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9330 - categorical_accuracy: 0.9330 - loss: 1.6853 - precision: 0.9375 - recall: 0.9146\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9079 - categorical_accuracy: 0.9079 - loss: 1.6950 - precision: 0.9226 - recall: 0.8926\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9213 - categorical_accuracy: 0.9213 - loss: 1.5650 - precision: 0.9304 - recall: 0.9188\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9563 - categorical_accuracy: 0.9563 - loss: 1.4433 - precision: 0.9595 - recall: 0.9470\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9641 - categorical_accuracy: 0.9641 - loss: 1.3565 - precision: 0.9662 - recall: 0.9593\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9420 - categorical_accuracy: 0.9420 - loss: 1.3252 - precision: 0.9522 - recall: 0.9326\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9342 - categorical_accuracy: 0.9342 - loss: 1.2902 - precision: 0.9462 - recall: 0.9287\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9552 - categorical_accuracy: 0.9552 - loss: 1.1833 - precision: 0.9594 - recall: 0.9472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2848 - categorical_accuracy: 0.2848 - loss: 7.2716 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6325 - categorical_accuracy: 0.6325 - loss: 5.5496 - precision: 0.3650 - recall: 0.0072     \n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7628 - categorical_accuracy: 0.7628 - loss: 4.0267 - precision: 0.9039 - recall: 0.4295\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8390 - categorical_accuracy: 0.8390 - loss: 3.2151 - precision: 0.8748 - recall: 0.7671\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8854 - categorical_accuracy: 0.8854 - loss: 2.8561 - precision: 0.9008 - recall: 0.8611\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8873 - categorical_accuracy: 0.8873 - loss: 2.5629 - precision: 0.9076 - recall: 0.8464\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9081 - categorical_accuracy: 0.9081 - loss: 2.2997 - precision: 0.9368 - recall: 0.8804\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9262 - categorical_accuracy: 0.9262 - loss: 2.1520 - precision: 0.9456 - recall: 0.9039\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9329 - categorical_accuracy: 0.9329 - loss: 1.9876 - precision: 0.9364 - recall: 0.9249\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9439 - categorical_accuracy: 0.9439 - loss: 1.8710 - precision: 0.9483 - recall: 0.9291\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8694 - categorical_accuracy: 0.8694 - loss: 1.9579 - precision: 0.8992 - recall: 0.8564\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9256 - categorical_accuracy: 0.9256 - loss: 1.7833 - precision: 0.9374 - recall: 0.9148\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9483 - categorical_accuracy: 0.9483 - loss: 1.6202 - precision: 0.9618 - recall: 0.9291\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9573 - categorical_accuracy: 0.9573 - loss: 1.4922 - precision: 0.9641 - recall: 0.9573\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9584 - categorical_accuracy: 0.9584 - loss: 1.4757 - precision: 0.9630 - recall: 0.9405\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9535 - categorical_accuracy: 0.9535 - loss: 1.4077 - precision: 0.9641 - recall: 0.9533\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9487 - categorical_accuracy: 0.9487 - loss: 1.3275 - precision: 0.9628 - recall: 0.9463\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9504 - categorical_accuracy: 0.9504 - loss: 1.2928 - precision: 0.9717 - recall: 0.9439\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9591 - categorical_accuracy: 0.9591 - loss: 1.2102 - precision: 0.9767 - recall: 0.9565\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9457 - categorical_accuracy: 0.9457 - loss: 1.2488 - precision: 0.9591 - recall: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.3090 - categorical_accuracy: 0.3090 - loss: 8.3106 - precision: 0.0333 - recall: 1.7153e-04   \n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5674 - categorical_accuracy: 0.5674 - loss: 5.6714 - precision: 0.7636 - recall: 0.2476\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7702 - categorical_accuracy: 0.7702 - loss: 4.0880 - precision: 0.8786 - recall: 0.5848\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8208 - categorical_accuracy: 0.8208 - loss: 3.3855 - precision: 0.8553 - recall: 0.7846\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8846 - categorical_accuracy: 0.8846 - loss: 2.9438 - precision: 0.9184 - recall: 0.8455\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8921 - categorical_accuracy: 0.8921 - loss: 2.6552 - precision: 0.9254 - recall: 0.8691\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8969 - categorical_accuracy: 0.8969 - loss: 2.4928 - precision: 0.9076 - recall: 0.8767\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9365 - categorical_accuracy: 0.9365 - loss: 2.2339 - precision: 0.9415 - recall: 0.9223\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9260 - categorical_accuracy: 0.9260 - loss: 2.1528 - precision: 0.9433 - recall: 0.9131\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9483 - categorical_accuracy: 0.9483 - loss: 2.0210 - precision: 0.9600 - recall: 0.9298\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9681 - categorical_accuracy: 0.9681 - loss: 1.8580 - precision: 0.9772 - recall: 0.9643\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9556 - categorical_accuracy: 0.9556 - loss: 1.7483 - precision: 0.9624 - recall: 0.9552\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9545 - categorical_accuracy: 0.9545 - loss: 1.6799 - precision: 0.9583 - recall: 0.9510\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9592 - categorical_accuracy: 0.9592 - loss: 1.5858 - precision: 0.9685 - recall: 0.9569\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9301 - categorical_accuracy: 0.9301 - loss: 1.6181 - precision: 0.9422 - recall: 0.9098\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9251 - categorical_accuracy: 0.9251 - loss: 1.5842 - precision: 0.9333 - recall: 0.9082\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9351 - categorical_accuracy: 0.9351 - loss: 1.4457 - precision: 0.9505 - recall: 0.9319\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9108 - categorical_accuracy: 0.9108 - loss: 1.4992 - precision: 0.9221 - recall: 0.8957\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9418 - categorical_accuracy: 0.9418 - loss: 1.3588 - precision: 0.9579 - recall: 0.9296\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9441 - categorical_accuracy: 0.9441 - loss: 1.3368 - precision: 0.9498 - recall: 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.2782 - categorical_accuracy: 0.2782 - loss: 8.3415 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5942 - categorical_accuracy: 0.5942 - loss: 5.6660 - precision: 0.8361 - recall: 0.2030\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7403 - categorical_accuracy: 0.7403 - loss: 4.0768 - precision: 0.8296 - recall: 0.6157\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8070 - categorical_accuracy: 0.8070 - loss: 3.3591 - precision: 0.8450 - recall: 0.7592\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8383 - categorical_accuracy: 0.8383 - loss: 2.9756 - precision: 0.8681 - recall: 0.8006\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8742 - categorical_accuracy: 0.8742 - loss: 2.6578 - precision: 0.9030 - recall: 0.8562\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8659 - categorical_accuracy: 0.8659 - loss: 2.5353 - precision: 0.8891 - recall: 0.8435\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8904 - categorical_accuracy: 0.8904 - loss: 2.3052 - precision: 0.9174 - recall: 0.8859\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9274 - categorical_accuracy: 0.9274 - loss: 2.1219 - precision: 0.9340 - recall: 0.9247\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8913 - categorical_accuracy: 0.8913 - loss: 2.0724 - precision: 0.8985 - recall: 0.8862\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9125 - categorical_accuracy: 0.9125 - loss: 1.9284 - precision: 0.9217 - recall: 0.8939\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9228 - categorical_accuracy: 0.9228 - loss: 1.8341 - precision: 0.9355 - recall: 0.9087\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9633 - categorical_accuracy: 0.9633 - loss: 1.6613 - precision: 0.9692 - recall: 0.9564\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9423 - categorical_accuracy: 0.9423 - loss: 1.6391 - precision: 0.9435 - recall: 0.9375\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9642 - categorical_accuracy: 0.9642 - loss: 1.5119 - precision: 0.9713 - recall: 0.9631\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9702 - categorical_accuracy: 0.9702 - loss: 1.4427 - precision: 0.9723 - recall: 0.9641\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9479 - categorical_accuracy: 0.9479 - loss: 1.4645 - precision: 0.9559 - recall: 0.9255\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9367 - categorical_accuracy: 0.9367 - loss: 1.4419 - precision: 0.9480 - recall: 0.9246\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9242 - categorical_accuracy: 0.9242 - loss: 1.4619 - precision: 0.9237 - recall: 0.9099\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9546 - categorical_accuracy: 0.9546 - loss: 1.2767 - precision: 0.9558 - recall: 0.9530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.2784 - categorical_accuracy: 0.2784 - loss: 8.3369 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6191 - categorical_accuracy: 0.6191 - loss: 5.7126 - precision: 0.8655 - recall: 0.1349\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7763 - categorical_accuracy: 0.7763 - loss: 4.1118 - precision: 0.8659 - recall: 0.5759\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7877 - categorical_accuracy: 0.7877 - loss: 3.4175 - precision: 0.8414 - recall: 0.7536\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8456 - categorical_accuracy: 0.8456 - loss: 3.0615 - precision: 0.8620 - recall: 0.7981\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8843 - categorical_accuracy: 0.8843 - loss: 2.7354 - precision: 0.9062 - recall: 0.8712\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8941 - categorical_accuracy: 0.8941 - loss: 2.5472 - precision: 0.9228 - recall: 0.8822\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9307 - categorical_accuracy: 0.9307 - loss: 2.3126 - precision: 0.9494 - recall: 0.9112\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9107 - categorical_accuracy: 0.9107 - loss: 2.1708 - precision: 0.9253 - recall: 0.8944\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9424 - categorical_accuracy: 0.9424 - loss: 2.0250 - precision: 0.9451 - recall: 0.9323\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8882 - categorical_accuracy: 0.8882 - loss: 2.0551 - precision: 0.9076 - recall: 0.8672\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9021 - categorical_accuracy: 0.9021 - loss: 1.9831 - precision: 0.9177 - recall: 0.8851\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9435 - categorical_accuracy: 0.9435 - loss: 1.7792 - precision: 0.9524 - recall: 0.9290\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9320 - categorical_accuracy: 0.9320 - loss: 1.7326 - precision: 0.9385 - recall: 0.9280\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9543 - categorical_accuracy: 0.9543 - loss: 1.6193 - precision: 0.9602 - recall: 0.9484\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9310 - categorical_accuracy: 0.9310 - loss: 1.6667 - precision: 0.9357 - recall: 0.9149\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9568 - categorical_accuracy: 0.9568 - loss: 1.4838 - precision: 0.9669 - recall: 0.9444\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9460 - categorical_accuracy: 0.9460 - loss: 1.4626 - precision: 0.9491 - recall: 0.9393\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9516 - categorical_accuracy: 0.9516 - loss: 1.4013 - precision: 0.9602 - recall: 0.9446\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9652 - categorical_accuracy: 0.9652 - loss: 1.2880 - precision: 0.9717 - recall: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.2758 - categorical_accuracy: 0.2758 - loss: 7.5089 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4408 - categorical_accuracy: 0.4408 - loss: 6.5873 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4823 - categorical_accuracy: 0.4823 - loss: 5.7158 - precision: 0.7790 - recall: 0.0534 \n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5426 - categorical_accuracy: 0.5426 - loss: 4.8767 - precision: 0.8431 - recall: 0.2664\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6703 - categorical_accuracy: 0.6703 - loss: 4.2737 - precision: 0.8785 - recall: 0.4114\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7866 - categorical_accuracy: 0.7866 - loss: 3.7401 - precision: 0.8815 - recall: 0.6494\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8476 - categorical_accuracy: 0.8476 - loss: 3.4048 - precision: 0.9042 - recall: 0.7430\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8694 - categorical_accuracy: 0.8694 - loss: 3.1786 - precision: 0.9164 - recall: 0.8265\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9032 - categorical_accuracy: 0.9032 - loss: 2.9903 - precision: 0.9161 - recall: 0.8814\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8824 - categorical_accuracy: 0.8824 - loss: 2.8942 - precision: 0.8956 - recall: 0.8563\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9393 - categorical_accuracy: 0.9393 - loss: 2.6650 - precision: 0.9475 - recall: 0.9128\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9350 - categorical_accuracy: 0.9350 - loss: 2.5529 - precision: 0.9418 - recall: 0.9243\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9489 - categorical_accuracy: 0.9489 - loss: 2.4302 - precision: 0.9584 - recall: 0.9457\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9503 - categorical_accuracy: 0.9503 - loss: 2.3376 - precision: 0.9595 - recall: 0.9426\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9601 - categorical_accuracy: 0.9601 - loss: 2.2474 - precision: 0.9688 - recall: 0.9342\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9277 - categorical_accuracy: 0.9277 - loss: 2.2693 - precision: 0.9368 - recall: 0.9069\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9389 - categorical_accuracy: 0.9389 - loss: 2.1831 - precision: 0.9438 - recall: 0.9384\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9453 - categorical_accuracy: 0.9453 - loss: 2.1510 - precision: 0.9552 - recall: 0.9345\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9610 - categorical_accuracy: 0.9610 - loss: 2.0110 - precision: 0.9752 - recall: 0.9582\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9600 - categorical_accuracy: 0.9600 - loss: 1.9857 - precision: 0.9662 - recall: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 6/10\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.2588 - categorical_accuracy: 0.2588 - loss: 7.4923 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4518 - categorical_accuracy: 0.4518 - loss: 6.5617 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5754 - categorical_accuracy: 0.5754 - loss: 5.6978 - precision: 0.2500 - recall: 0.0011      \n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6420 - categorical_accuracy: 0.6420 - loss: 4.7659 - precision: 0.8596 - recall: 0.2800\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6753 - categorical_accuracy: 0.6753 - loss: 4.1130 - precision: 0.8318 - recall: 0.4138\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7974 - categorical_accuracy: 0.7974 - loss: 3.5732 - precision: 0.8653 - recall: 0.6478\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8426 - categorical_accuracy: 0.8426 - loss: 3.2494 - precision: 0.8658 - recall: 0.7572\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8305 - categorical_accuracy: 0.8305 - loss: 3.0702 - precision: 0.8492 - recall: 0.7919\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8997 - categorical_accuracy: 0.8997 - loss: 2.8149 - precision: 0.9180 - recall: 0.8728\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9123 - categorical_accuracy: 0.9123 - loss: 2.6362 - precision: 0.9170 - recall: 0.8989\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9333 - categorical_accuracy: 0.9333 - loss: 2.4971 - precision: 0.9402 - recall: 0.9129\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9481 - categorical_accuracy: 0.9481 - loss: 2.3508 - precision: 0.9626 - recall: 0.9309\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9366 - categorical_accuracy: 0.9366 - loss: 2.3156 - precision: 0.9517 - recall: 0.9100\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9576 - categorical_accuracy: 0.9576 - loss: 2.1607 - precision: 0.9597 - recall: 0.9504\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9420 - categorical_accuracy: 0.9420 - loss: 2.1258 - precision: 0.9452 - recall: 0.9365\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9673 - categorical_accuracy: 0.9673 - loss: 2.0071 - precision: 0.9728 - recall: 0.9517\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9614 - categorical_accuracy: 0.9614 - loss: 1.9618 - precision: 0.9659 - recall: 0.9546\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9407 - categorical_accuracy: 0.9407 - loss: 1.9382 - precision: 0.9534 - recall: 0.9314\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9470 - categorical_accuracy: 0.9470 - loss: 1.8736 - precision: 0.9554 - recall: 0.9378\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9601 - categorical_accuracy: 0.9601 - loss: 1.7956 - precision: 0.9622 - recall: 0.9559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.2590 - categorical_accuracy: 0.2590 - loss: 7.4802 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4717 - categorical_accuracy: 0.4717 - loss: 6.5584 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5905 - categorical_accuracy: 0.5905 - loss: 5.6990 - precision: 0.7698 - recall: 0.0474  \n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7469 - categorical_accuracy: 0.7469 - loss: 4.8135 - precision: 0.8856 - recall: 0.2409\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8219 - categorical_accuracy: 0.8219 - loss: 4.0806 - precision: 0.8788 - recall: 0.5589\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9034 - categorical_accuracy: 0.9034 - loss: 3.5307 - precision: 0.9337 - recall: 0.8071\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9045 - categorical_accuracy: 0.9045 - loss: 3.2006 - precision: 0.9275 - recall: 0.8923\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9202 - categorical_accuracy: 0.9202 - loss: 2.9811 - precision: 0.9402 - recall: 0.9095\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9144 - categorical_accuracy: 0.9144 - loss: 2.8299 - precision: 0.9384 - recall: 0.9076\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9465 - categorical_accuracy: 0.9465 - loss: 2.6247 - precision: 0.9514 - recall: 0.9388\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9519 - categorical_accuracy: 0.9519 - loss: 2.4817 - precision: 0.9571 - recall: 0.9406\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9596 - categorical_accuracy: 0.9596 - loss: 2.3650 - precision: 0.9627 - recall: 0.9563\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9600 - categorical_accuracy: 0.9600 - loss: 2.2673 - precision: 0.9623 - recall: 0.9543\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9710 - categorical_accuracy: 0.9710 - loss: 2.1519 - precision: 0.9709 - recall: 0.9674\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9572 - categorical_accuracy: 0.9572 - loss: 2.0914 - precision: 0.9704 - recall: 0.9482\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9722 - categorical_accuracy: 0.9722 - loss: 1.9977 - precision: 0.9851 - recall: 0.9696\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9726 - categorical_accuracy: 0.9726 - loss: 1.9208 - precision: 0.9871 - recall: 0.9726\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9764 - categorical_accuracy: 0.9764 - loss: 1.8765 - precision: 0.9776 - recall: 0.9706\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9865 - categorical_accuracy: 0.9865 - loss: 1.7763 - precision: 0.9892 - recall: 0.9849\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9858 - categorical_accuracy: 0.9858 - loss: 1.7134 - precision: 0.9896 - recall: 0.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.2412 - categorical_accuracy: 0.2412 - loss: 8.6966 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5143 - categorical_accuracy: 0.5143 - loss: 7.1728 - precision: 0.4125 - recall: 0.0107    \n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6747 - categorical_accuracy: 0.6747 - loss: 5.7692 - precision: 0.8363 - recall: 0.2770\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7929 - categorical_accuracy: 0.7929 - loss: 4.7552 - precision: 0.9180 - recall: 0.5905\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8705 - categorical_accuracy: 0.8705 - loss: 4.0891 - precision: 0.9065 - recall: 0.8316\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8939 - categorical_accuracy: 0.8939 - loss: 3.6765 - precision: 0.9088 - recall: 0.8823\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9207 - categorical_accuracy: 0.9207 - loss: 3.3821 - precision: 0.9354 - recall: 0.9016\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9319 - categorical_accuracy: 0.9319 - loss: 3.1507 - precision: 0.9505 - recall: 0.9172\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9439 - categorical_accuracy: 0.9439 - loss: 2.9592 - precision: 0.9552 - recall: 0.9379\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9604 - categorical_accuracy: 0.9604 - loss: 2.7931 - precision: 0.9693 - recall: 0.9513\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9576 - categorical_accuracy: 0.9576 - loss: 2.6912 - precision: 0.9675 - recall: 0.9475\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9364 - categorical_accuracy: 0.9364 - loss: 2.6574 - precision: 0.9525 - recall: 0.9187\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9683 - categorical_accuracy: 0.9683 - loss: 2.4826 - precision: 0.9755 - recall: 0.9501\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9495 - categorical_accuracy: 0.9495 - loss: 2.4448 - precision: 0.9576 - recall: 0.9359\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9590 - categorical_accuracy: 0.9590 - loss: 2.3291 - precision: 0.9699 - recall: 0.9570\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9645 - categorical_accuracy: 0.9645 - loss: 2.2577 - precision: 0.9724 - recall: 0.9581\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9815 - categorical_accuracy: 0.9815 - loss: 2.1735 - precision: 0.9870 - recall: 0.9704\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9652 - categorical_accuracy: 0.9652 - loss: 2.1277 - precision: 0.9676 - recall: 0.9545\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9621 - categorical_accuracy: 0.9621 - loss: 2.1105 - precision: 0.9711 - recall: 0.9547\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9780 - categorical_accuracy: 0.9780 - loss: 2.0084 - precision: 0.9777 - recall: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 6/10\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.2341 - categorical_accuracy: 0.2341 - loss: 8.6981 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5877 - categorical_accuracy: 0.5877 - loss: 7.2018 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6886 - categorical_accuracy: 0.6886 - loss: 5.7977 - precision: 0.8389 - recall: 0.1467\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8143 - categorical_accuracy: 0.8143 - loss: 4.6618 - precision: 0.8889 - recall: 0.6966\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8463 - categorical_accuracy: 0.8463 - loss: 4.1081 - precision: 0.8737 - recall: 0.7953\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8726 - categorical_accuracy: 0.8726 - loss: 3.7335 - precision: 0.8873 - recall: 0.8321\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8987 - categorical_accuracy: 0.8987 - loss: 3.4135 - precision: 0.9290 - recall: 0.8829\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9012 - categorical_accuracy: 0.9012 - loss: 3.2269 - precision: 0.9274 - recall: 0.8883\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9158 - categorical_accuracy: 0.9158 - loss: 3.0469 - precision: 0.9360 - recall: 0.9018\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9440 - categorical_accuracy: 0.9440 - loss: 2.8500 - precision: 0.9534 - recall: 0.9385\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9715 - categorical_accuracy: 0.9715 - loss: 2.6968 - precision: 0.9788 - recall: 0.9579\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9637 - categorical_accuracy: 0.9637 - loss: 2.5792 - precision: 0.9664 - recall: 0.9613\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9458 - categorical_accuracy: 0.9458 - loss: 2.5854 - precision: 0.9481 - recall: 0.9256\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9596 - categorical_accuracy: 0.9596 - loss: 2.4504 - precision: 0.9636 - recall: 0.9519\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9539 - categorical_accuracy: 0.9539 - loss: 2.4184 - precision: 0.9595 - recall: 0.9429\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9758 - categorical_accuracy: 0.9758 - loss: 2.2746 - precision: 0.9773 - recall: 0.9732\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9570 - categorical_accuracy: 0.9570 - loss: 2.2301 - precision: 0.9602 - recall: 0.9522\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9665 - categorical_accuracy: 0.9665 - loss: 2.1598 - precision: 0.9664 - recall: 0.9639\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9640 - categorical_accuracy: 0.9640 - loss: 2.1110 - precision: 0.9673 - recall: 0.9580\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9603 - categorical_accuracy: 0.9603 - loss: 2.0938 - precision: 0.9598 - recall: 0.9498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - accuracy: 0.2809 - categorical_accuracy: 0.2809 - loss: 8.6987 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5046 - categorical_accuracy: 0.5046 - loss: 7.2224 - precision: 0.3500 - recall: 0.0067     \n",
      "Epoch 3/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6054 - categorical_accuracy: 0.6054 - loss: 5.8716 - precision: 0.8772 - recall: 0.3085\n",
      "Epoch 4/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7108 - categorical_accuracy: 0.7108 - loss: 4.9297 - precision: 0.8894 - recall: 0.5227\n",
      "Epoch 5/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8065 - categorical_accuracy: 0.8065 - loss: 4.1790 - precision: 0.8918 - recall: 0.7134\n",
      "Epoch 6/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8889 - categorical_accuracy: 0.8889 - loss: 3.6680 - precision: 0.9219 - recall: 0.8580\n",
      "Epoch 7/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9092 - categorical_accuracy: 0.9092 - loss: 3.3610 - precision: 0.9254 - recall: 0.8714\n",
      "Epoch 8/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8954 - categorical_accuracy: 0.8954 - loss: 3.1467 - precision: 0.9288 - recall: 0.8719\n",
      "Epoch 9/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9198 - categorical_accuracy: 0.9198 - loss: 2.9901 - precision: 0.9336 - recall: 0.9014\n",
      "Epoch 10/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9416 - categorical_accuracy: 0.9416 - loss: 2.8095 - precision: 0.9532 - recall: 0.9198\n",
      "Epoch 11/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9695 - categorical_accuracy: 0.9695 - loss: 2.6314 - precision: 0.9725 - recall: 0.9557\n",
      "Epoch 12/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9680 - categorical_accuracy: 0.9680 - loss: 2.5329 - precision: 0.9766 - recall: 0.9562\n",
      "Epoch 13/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9494 - categorical_accuracy: 0.9494 - loss: 2.4786 - precision: 0.9534 - recall: 0.9399\n",
      "Epoch 14/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9478 - categorical_accuracy: 0.9478 - loss: 2.4130 - precision: 0.9576 - recall: 0.9409\n",
      "Epoch 15/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9669 - categorical_accuracy: 0.9669 - loss: 2.3110 - precision: 0.9689 - recall: 0.9486\n",
      "Epoch 16/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9695 - categorical_accuracy: 0.9695 - loss: 2.2059 - precision: 0.9804 - recall: 0.9686\n",
      "Epoch 17/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9677 - categorical_accuracy: 0.9677 - loss: 2.1435 - precision: 0.9780 - recall: 0.9621\n",
      "Epoch 18/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9919 - categorical_accuracy: 0.9919 - loss: 2.0273 - precision: 0.9924 - recall: 0.9919\n",
      "Epoch 19/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9864 - categorical_accuracy: 0.9864 - loss: 1.9824 - precision: 0.9863 - recall: 0.9818\n",
      "Epoch 20/20\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9722 - categorical_accuracy: 0.9722 - loss: 1.9550 - precision: 0.9744 - recall: 0.9689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.2522 - categorical_accuracy: 0.2522 - loss: 7.4480 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4793 - categorical_accuracy: 0.4793 - loss: 6.3357 - precision: 0.3636 - recall: 0.0031      \n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4903 - categorical_accuracy: 0.4903 - loss: 5.3256 - precision: 0.6722 - recall: 0.1567\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5947 - categorical_accuracy: 0.5947 - loss: 4.4418 - precision: 0.8032 - recall: 0.2793\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7446 - categorical_accuracy: 0.7446 - loss: 3.7932 - precision: 0.8434 - recall: 0.4835\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8526 - categorical_accuracy: 0.8526 - loss: 3.3557 - precision: 0.8777 - recall: 0.7744\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8698 - categorical_accuracy: 0.8698 - loss: 3.0491 - precision: 0.8979 - recall: 0.8451\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8908 - categorical_accuracy: 0.8908 - loss: 2.8418 - precision: 0.9064 - recall: 0.8675\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9032 - categorical_accuracy: 0.9032 - loss: 2.7079 - precision: 0.9207 - recall: 0.8629\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9200 - categorical_accuracy: 0.9200 - loss: 2.5531 - precision: 0.9331 - recall: 0.8911\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9384 - categorical_accuracy: 0.9384 - loss: 2.4224 - precision: 0.9503 - recall: 0.9240\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9248 - categorical_accuracy: 0.9248 - loss: 2.3903 - precision: 0.9328 - recall: 0.9058\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9308 - categorical_accuracy: 0.9308 - loss: 2.2975 - precision: 0.9384 - recall: 0.9125\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9525 - categorical_accuracy: 0.9525 - loss: 2.2074 - precision: 0.9572 - recall: 0.9356\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9578 - categorical_accuracy: 0.9578 - loss: 2.1424 - precision: 0.9662 - recall: 0.9438\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9687 - categorical_accuracy: 0.9687 - loss: 2.0302 - precision: 0.9737 - recall: 0.9582\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9851 - categorical_accuracy: 0.9851 - loss: 1.9345 - precision: 0.9851 - recall: 0.9839\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9844 - categorical_accuracy: 0.9844 - loss: 1.8803 - precision: 0.9842 - recall: 0.9749\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9794 - categorical_accuracy: 0.9794 - loss: 1.8311 - precision: 0.9850 - recall: 0.9740\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9801 - categorical_accuracy: 0.9801 - loss: 1.7487 - precision: 0.9822 - recall: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 324ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.2525 - categorical_accuracy: 0.2525 - loss: 7.4718 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5038 - categorical_accuracy: 0.5038 - loss: 6.3807 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6562 - categorical_accuracy: 0.6562 - loss: 5.3953 - precision: 0.4311 - recall: 0.0065      \n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7682 - categorical_accuracy: 0.7682 - loss: 4.4511 - precision: 0.9138 - recall: 0.2123\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8212 - categorical_accuracy: 0.8212 - loss: 3.6109 - precision: 0.9145 - recall: 0.6014\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8891 - categorical_accuracy: 0.8891 - loss: 3.1212 - precision: 0.9058 - recall: 0.8549\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8862 - categorical_accuracy: 0.8862 - loss: 2.8876 - precision: 0.9074 - recall: 0.8643\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9091 - categorical_accuracy: 0.9091 - loss: 2.6859 - precision: 0.9233 - recall: 0.8934\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9269 - categorical_accuracy: 0.9269 - loss: 2.5257 - precision: 0.9378 - recall: 0.9008\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9438 - categorical_accuracy: 0.9438 - loss: 2.4044 - precision: 0.9517 - recall: 0.9275\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9341 - categorical_accuracy: 0.9341 - loss: 2.3018 - precision: 0.9499 - recall: 0.9178\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9416 - categorical_accuracy: 0.9416 - loss: 2.1947 - precision: 0.9543 - recall: 0.9284\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9535 - categorical_accuracy: 0.9535 - loss: 2.0901 - precision: 0.9638 - recall: 0.9440\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9327 - categorical_accuracy: 0.9327 - loss: 2.0954 - precision: 0.9363 - recall: 0.9157\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9404 - categorical_accuracy: 0.9404 - loss: 2.0220 - precision: 0.9444 - recall: 0.9158\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9499 - categorical_accuracy: 0.9499 - loss: 1.9397 - precision: 0.9543 - recall: 0.9314\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9488 - categorical_accuracy: 0.9488 - loss: 1.8821 - precision: 0.9625 - recall: 0.9420\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9581 - categorical_accuracy: 0.9581 - loss: 1.8209 - precision: 0.9704 - recall: 0.9435\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9048 - categorical_accuracy: 0.9048 - loss: 1.8855 - precision: 0.9190 - recall: 0.8815\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9282 - categorical_accuracy: 0.9282 - loss: 1.8125 - precision: 0.9541 - recall: 0.9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 332ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.2558 - categorical_accuracy: 0.2558 - loss: 7.4646 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4603 - categorical_accuracy: 0.4603 - loss: 6.3627 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5815 - categorical_accuracy: 0.5815 - loss: 5.3525 - precision: 0.8112 - recall: 0.0815   \n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7171 - categorical_accuracy: 0.7171 - loss: 4.4164 - precision: 0.9158 - recall: 0.3575\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8608 - categorical_accuracy: 0.8608 - loss: 3.6641 - precision: 0.9238 - recall: 0.6069\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8610 - categorical_accuracy: 0.8610 - loss: 3.2425 - precision: 0.8839 - recall: 0.8198\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8772 - categorical_accuracy: 0.8772 - loss: 2.9528 - precision: 0.8877 - recall: 0.8516\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8927 - categorical_accuracy: 0.8927 - loss: 2.7596 - precision: 0.9191 - recall: 0.8723\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9053 - categorical_accuracy: 0.9053 - loss: 2.6339 - precision: 0.9098 - recall: 0.8909\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9226 - categorical_accuracy: 0.9226 - loss: 2.4497 - precision: 0.9360 - recall: 0.9067\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9400 - categorical_accuracy: 0.9400 - loss: 2.3066 - precision: 0.9494 - recall: 0.9323\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9600 - categorical_accuracy: 0.9600 - loss: 2.1501 - precision: 0.9705 - recall: 0.9568\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9640 - categorical_accuracy: 0.9640 - loss: 2.0576 - precision: 0.9660 - recall: 0.9592\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9289 - categorical_accuracy: 0.9289 - loss: 2.0597 - precision: 0.9400 - recall: 0.9262\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9732 - categorical_accuracy: 0.9732 - loss: 1.8727 - precision: 0.9739 - recall: 0.9732\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9651 - categorical_accuracy: 0.9651 - loss: 1.8385 - precision: 0.9653 - recall: 0.9637\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9428 - categorical_accuracy: 0.9428 - loss: 1.8057 - precision: 0.9498 - recall: 0.9317\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9395 - categorical_accuracy: 0.9395 - loss: 1.7920 - precision: 0.9497 - recall: 0.9334\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9556 - categorical_accuracy: 0.9556 - loss: 1.7001 - precision: 0.9582 - recall: 0.9488\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9658 - categorical_accuracy: 0.9658 - loss: 1.6370 - precision: 0.9731 - recall: 0.9572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.3127 - categorical_accuracy: 0.3127 - loss: 8.6329 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5465 - categorical_accuracy: 0.5465 - loss: 6.8506 - precision: 0.7084 - recall: 0.0674  \n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6315 - categorical_accuracy: 0.6315 - loss: 5.4321 - precision: 0.7381 - recall: 0.3099\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7026 - categorical_accuracy: 0.7026 - loss: 4.4483 - precision: 0.8052 - recall: 0.6343\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7700 - categorical_accuracy: 0.7700 - loss: 3.8090 - precision: 0.8742 - recall: 0.6680\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8268 - categorical_accuracy: 0.8268 - loss: 3.3604 - precision: 0.8582 - recall: 0.7523\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8144 - categorical_accuracy: 0.8144 - loss: 3.2175 - precision: 0.8653 - recall: 0.7789\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8726 - categorical_accuracy: 0.8726 - loss: 2.9810 - precision: 0.9166 - recall: 0.7969\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8684 - categorical_accuracy: 0.8684 - loss: 2.8381 - precision: 0.8962 - recall: 0.8222\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8847 - categorical_accuracy: 0.8847 - loss: 2.7125 - precision: 0.9107 - recall: 0.8533\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8974 - categorical_accuracy: 0.8974 - loss: 2.6097 - precision: 0.9109 - recall: 0.8795\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8903 - categorical_accuracy: 0.8903 - loss: 2.5370 - precision: 0.9111 - recall: 0.8769\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8947 - categorical_accuracy: 0.8947 - loss: 2.4613 - precision: 0.9132 - recall: 0.8809\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9037 - categorical_accuracy: 0.9037 - loss: 2.3771 - precision: 0.9288 - recall: 0.8790\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9364 - categorical_accuracy: 0.9364 - loss: 2.2708 - precision: 0.9423 - recall: 0.9194\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9293 - categorical_accuracy: 0.9293 - loss: 2.2272 - precision: 0.9404 - recall: 0.8985\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9157 - categorical_accuracy: 0.9157 - loss: 2.1937 - precision: 0.9331 - recall: 0.8936\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9286 - categorical_accuracy: 0.9286 - loss: 2.1531 - precision: 0.9507 - recall: 0.9016\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9341 - categorical_accuracy: 0.9341 - loss: 2.0641 - precision: 0.9416 - recall: 0.9259\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9339 - categorical_accuracy: 0.9339 - loss: 2.0552 - precision: 0.9441 - recall: 0.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 358ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.2881 - categorical_accuracy: 0.2881 - loss: 8.6613 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5984 - categorical_accuracy: 0.5984 - loss: 6.9116 - precision: 0.6130 - recall: 0.0316   \n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7170 - categorical_accuracy: 0.7170 - loss: 5.3994 - precision: 0.9178 - recall: 0.4269\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8190 - categorical_accuracy: 0.8190 - loss: 4.3735 - precision: 0.9292 - recall: 0.6083\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8246 - categorical_accuracy: 0.8246 - loss: 3.8609 - precision: 0.8487 - recall: 0.7611\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8491 - categorical_accuracy: 0.8491 - loss: 3.5052 - precision: 0.8612 - recall: 0.8089\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8839 - categorical_accuracy: 0.8839 - loss: 3.1849 - precision: 0.8985 - recall: 0.8687\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9015 - categorical_accuracy: 0.9015 - loss: 2.9934 - precision: 0.9163 - recall: 0.8773\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9235 - categorical_accuracy: 0.9235 - loss: 2.7667 - precision: 0.9376 - recall: 0.9163\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9401 - categorical_accuracy: 0.9401 - loss: 2.6368 - precision: 0.9467 - recall: 0.9340\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9368 - categorical_accuracy: 0.9368 - loss: 2.5147 - precision: 0.9494 - recall: 0.9271\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9265 - categorical_accuracy: 0.9265 - loss: 2.4491 - precision: 0.9379 - recall: 0.9130\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9589 - categorical_accuracy: 0.9589 - loss: 2.2838 - precision: 0.9724 - recall: 0.9489\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9653 - categorical_accuracy: 0.9653 - loss: 2.2000 - precision: 0.9685 - recall: 0.9537\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9541 - categorical_accuracy: 0.9541 - loss: 2.1494 - precision: 0.9695 - recall: 0.9436\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9304 - categorical_accuracy: 0.9304 - loss: 2.1542 - precision: 0.9377 - recall: 0.9237\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9593 - categorical_accuracy: 0.9593 - loss: 2.0504 - precision: 0.9726 - recall: 0.9424\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9463 - categorical_accuracy: 0.9463 - loss: 2.0438 - precision: 0.9519 - recall: 0.9366\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9164 - categorical_accuracy: 0.9164 - loss: 2.0525 - precision: 0.9319 - recall: 0.8984\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9272 - categorical_accuracy: 0.9272 - loss: 1.9932 - precision: 0.9395 - recall: 0.9127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.2537 - categorical_accuracy: 0.2537 - loss: 8.6496 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5748 - categorical_accuracy: 0.5748 - loss: 6.9231 - precision: 0.4354 - recall: 0.0075     \n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.6818 - categorical_accuracy: 0.6818 - loss: 5.4538 - precision: 0.8929 - recall: 0.2804\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7846 - categorical_accuracy: 0.7846 - loss: 4.3563 - precision: 0.9078 - recall: 0.5621\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8072 - categorical_accuracy: 0.8072 - loss: 3.7881 - precision: 0.8384 - recall: 0.7632\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8659 - categorical_accuracy: 0.8659 - loss: 3.3549 - precision: 0.8940 - recall: 0.8423\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8962 - categorical_accuracy: 0.8962 - loss: 3.0856 - precision: 0.9282 - recall: 0.8494\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9046 - categorical_accuracy: 0.9046 - loss: 2.9069 - precision: 0.9124 - recall: 0.8900\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9179 - categorical_accuracy: 0.9179 - loss: 2.7562 - precision: 0.9317 - recall: 0.8829\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9228 - categorical_accuracy: 0.9228 - loss: 2.5793 - precision: 0.9355 - recall: 0.9136\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9592 - categorical_accuracy: 0.9592 - loss: 2.4420 - precision: 0.9613 - recall: 0.9451\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9401 - categorical_accuracy: 0.9401 - loss: 2.3692 - precision: 0.9458 - recall: 0.9343\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9484 - categorical_accuracy: 0.9484 - loss: 2.2497 - precision: 0.9648 - recall: 0.9382\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9416 - categorical_accuracy: 0.9416 - loss: 2.2605 - precision: 0.9457 - recall: 0.9310\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8889 - categorical_accuracy: 0.8889 - loss: 2.3355 - precision: 0.9039 - recall: 0.8433\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9328 - categorical_accuracy: 0.9328 - loss: 2.1732 - precision: 0.9519 - recall: 0.9051\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9515 - categorical_accuracy: 0.9515 - loss: 2.0604 - precision: 0.9715 - recall: 0.9438\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9289 - categorical_accuracy: 0.9289 - loss: 2.0645 - precision: 0.9379 - recall: 0.9190\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9433 - categorical_accuracy: 0.9433 - loss: 2.0344 - precision: 0.9567 - recall: 0.9322\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9771 - categorical_accuracy: 0.9771 - loss: 1.9127 - precision: 0.9831 - recall: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.2201 - categorical_accuracy: 0.2201 - loss: 7.5944 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.4139 - categorical_accuracy: 0.4139 - loss: 7.0191 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5046 - categorical_accuracy: 0.5046 - loss: 6.4984 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6031 - categorical_accuracy: 0.6031 - loss: 5.9858 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6332 - categorical_accuracy: 0.6332 - loss: 5.4620 - precision: 0.5035 - recall: 0.0071     \n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6918 - categorical_accuracy: 0.6918 - loss: 4.9069 - precision: 0.9124 - recall: 0.1846\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.7403 - categorical_accuracy: 0.7403 - loss: 4.3844 - precision: 0.9259 - recall: 0.6148\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7734 - categorical_accuracy: 0.7734 - loss: 4.0114 - precision: 0.8809 - recall: 0.7181\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7718 - categorical_accuracy: 0.7718 - loss: 3.7411 - precision: 0.9081 - recall: 0.7291\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8410 - categorical_accuracy: 0.8410 - loss: 3.5350 - precision: 0.9084 - recall: 0.7485\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8887 - categorical_accuracy: 0.8887 - loss: 3.3429 - precision: 0.9157 - recall: 0.7881\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8940 - categorical_accuracy: 0.8940 - loss: 3.1885 - precision: 0.9157 - recall: 0.8665\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9324 - categorical_accuracy: 0.9324 - loss: 3.0201 - precision: 0.9463 - recall: 0.8964\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9179 - categorical_accuracy: 0.9179 - loss: 2.9309 - precision: 0.9336 - recall: 0.9067\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9069 - categorical_accuracy: 0.9069 - loss: 2.8527 - precision: 0.9182 - recall: 0.8949\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9211 - categorical_accuracy: 0.9211 - loss: 2.7836 - precision: 0.9331 - recall: 0.9110\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9332 - categorical_accuracy: 0.9332 - loss: 2.7070 - precision: 0.9409 - recall: 0.9171\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9657 - categorical_accuracy: 0.9657 - loss: 2.5614 - precision: 0.9746 - recall: 0.9571\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9733 - categorical_accuracy: 0.9733 - loss: 2.4785 - precision: 0.9766 - recall: 0.9653\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9568 - categorical_accuracy: 0.9568 - loss: 2.4571 - precision: 0.9655 - recall: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.2178 - categorical_accuracy: 0.2178 - loss: 7.5820 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.3320 - categorical_accuracy: 0.3320 - loss: 7.0190 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.4484 - categorical_accuracy: 0.4484 - loss: 6.5089 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5624 - categorical_accuracy: 0.5624 - loss: 6.0211 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6318 - categorical_accuracy: 0.6318 - loss: 5.5339 - precision: 0.4545 - recall: 0.0027     \n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7257 - categorical_accuracy: 0.7257 - loss: 5.0115 - precision: 0.9918 - recall: 0.1545\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7586 - categorical_accuracy: 0.7586 - loss: 4.5058 - precision: 0.9666 - recall: 0.3506\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8376 - categorical_accuracy: 0.8376 - loss: 4.0622 - precision: 0.9225 - recall: 0.4654\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8305 - categorical_accuracy: 0.8305 - loss: 3.6994 - precision: 0.9038 - recall: 0.7231\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8500 - categorical_accuracy: 0.8500 - loss: 3.4854 - precision: 0.8832 - recall: 0.8034\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9145 - categorical_accuracy: 0.9145 - loss: 3.2485 - precision: 0.9339 - recall: 0.8812\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9158 - categorical_accuracy: 0.9158 - loss: 3.1008 - precision: 0.9422 - recall: 0.9054\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9410 - categorical_accuracy: 0.9410 - loss: 2.9202 - precision: 0.9525 - recall: 0.9339\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9560 - categorical_accuracy: 0.9560 - loss: 2.8028 - precision: 0.9623 - recall: 0.9515\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9410 - categorical_accuracy: 0.9410 - loss: 2.7517 - precision: 0.9479 - recall: 0.9310\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9508 - categorical_accuracy: 0.9508 - loss: 2.6573 - precision: 0.9550 - recall: 0.9481\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9463 - categorical_accuracy: 0.9463 - loss: 2.5698 - precision: 0.9637 - recall: 0.9281\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9466 - categorical_accuracy: 0.9466 - loss: 2.5096 - precision: 0.9678 - recall: 0.9365\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9421 - categorical_accuracy: 0.9421 - loss: 2.4755 - precision: 0.9471 - recall: 0.9246\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9691 - categorical_accuracy: 0.9691 - loss: 2.3720 - precision: 0.9742 - recall: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 327ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.2456 - categorical_accuracy: 0.2456 - loss: 7.6044 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.3739 - categorical_accuracy: 0.3739 - loss: 7.0374 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5104 - categorical_accuracy: 0.5104 - loss: 6.5157 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5401 - categorical_accuracy: 0.5401 - loss: 5.9957 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5481 - categorical_accuracy: 0.5481 - loss: 5.4515 - precision: 0.5455 - recall: 0.0072    \n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5180 - categorical_accuracy: 0.5180 - loss: 4.9294 - precision: 0.7778 - recall: 0.1977\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.6013 - categorical_accuracy: 0.6013 - loss: 4.4947 - precision: 0.7807 - recall: 0.3381\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8181 - categorical_accuracy: 0.8181 - loss: 4.0837 - precision: 0.9125 - recall: 0.5007\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8201 - categorical_accuracy: 0.8201 - loss: 3.7571 - precision: 0.8838 - recall: 0.6374\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8803 - categorical_accuracy: 0.8803 - loss: 3.4668 - precision: 0.9217 - recall: 0.7756\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9278 - categorical_accuracy: 0.9278 - loss: 3.1859 - precision: 0.9393 - recall: 0.8935\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9291 - categorical_accuracy: 0.9291 - loss: 3.0406 - precision: 0.9459 - recall: 0.8973\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9248 - categorical_accuracy: 0.9248 - loss: 2.9193 - precision: 0.9354 - recall: 0.9128\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9251 - categorical_accuracy: 0.9251 - loss: 2.8196 - precision: 0.9373 - recall: 0.9041\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9415 - categorical_accuracy: 0.9415 - loss: 2.6970 - precision: 0.9439 - recall: 0.9294\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9594 - categorical_accuracy: 0.9594 - loss: 2.5915 - precision: 0.9658 - recall: 0.9456\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9633 - categorical_accuracy: 0.9633 - loss: 2.5034 - precision: 0.9630 - recall: 0.9540\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9246 - categorical_accuracy: 0.9246 - loss: 2.5309 - precision: 0.9317 - recall: 0.9150\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9560 - categorical_accuracy: 0.9560 - loss: 2.3977 - precision: 0.9604 - recall: 0.9443\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9747 - categorical_accuracy: 0.9747 - loss: 2.3099 - precision: 0.9815 - recall: 0.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 339ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - accuracy: 0.2641 - categorical_accuracy: 0.2641 - loss: 8.8775 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5124 - categorical_accuracy: 0.5124 - loss: 7.9492 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5855 - categorical_accuracy: 0.5855 - loss: 7.0911 - precision: 0.4545 - recall: 0.0043     \n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5186 - categorical_accuracy: 0.5186 - loss: 6.3001 - precision: 0.7042 - recall: 0.1208\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6354 - categorical_accuracy: 0.6354 - loss: 5.6274 - precision: 0.7774 - recall: 0.3188\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7381 - categorical_accuracy: 0.7381 - loss: 5.0377 - precision: 0.8467 - recall: 0.5886\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8470 - categorical_accuracy: 0.8470 - loss: 4.5778 - precision: 0.8902 - recall: 0.6746\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8609 - categorical_accuracy: 0.8609 - loss: 4.2316 - precision: 0.8848 - recall: 0.8078\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8938 - categorical_accuracy: 0.8938 - loss: 3.9529 - precision: 0.9148 - recall: 0.8560\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9005 - categorical_accuracy: 0.9005 - loss: 3.7303 - precision: 0.9149 - recall: 0.8701\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9363 - categorical_accuracy: 0.9363 - loss: 3.5175 - precision: 0.9481 - recall: 0.9115\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9462 - categorical_accuracy: 0.9462 - loss: 3.3716 - precision: 0.9511 - recall: 0.9167\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9488 - categorical_accuracy: 0.9488 - loss: 3.2329 - precision: 0.9625 - recall: 0.9337\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9128 - categorical_accuracy: 0.9128 - loss: 3.1703 - precision: 0.9225 - recall: 0.9035\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9407 - categorical_accuracy: 0.9407 - loss: 3.0628 - precision: 0.9511 - recall: 0.9289\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9495 - categorical_accuracy: 0.9495 - loss: 2.9437 - precision: 0.9552 - recall: 0.9438\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9618 - categorical_accuracy: 0.9618 - loss: 2.8576 - precision: 0.9689 - recall: 0.9504\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9341 - categorical_accuracy: 0.9341 - loss: 2.8337 - precision: 0.9506 - recall: 0.9303\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9546 - categorical_accuracy: 0.9546 - loss: 2.7591 - precision: 0.9744 - recall: 0.9378\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9826 - categorical_accuracy: 0.9826 - loss: 2.6384 - precision: 0.9857 - recall: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 339ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.2375 - categorical_accuracy: 0.2375 - loss: 8.8649 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5088 - categorical_accuracy: 0.5088 - loss: 7.9477 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5534 - categorical_accuracy: 0.5534 - loss: 7.1043 - precision: 0.3636 - recall: 0.0016      \n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6040 - categorical_accuracy: 0.6040 - loss: 6.2670 - precision: 0.8026 - recall: 0.1847\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6753 - categorical_accuracy: 0.6753 - loss: 5.5619 - precision: 0.8352 - recall: 0.4287\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7966 - categorical_accuracy: 0.7966 - loss: 4.9253 - precision: 0.9253 - recall: 0.5724\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8167 - categorical_accuracy: 0.8167 - loss: 4.4951 - precision: 0.9172 - recall: 0.6905\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8478 - categorical_accuracy: 0.8478 - loss: 4.1810 - precision: 0.8958 - recall: 0.7500\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8936 - categorical_accuracy: 0.8936 - loss: 3.9203 - precision: 0.9260 - recall: 0.8230\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8945 - categorical_accuracy: 0.8945 - loss: 3.7096 - precision: 0.9271 - recall: 0.8583\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8760 - categorical_accuracy: 0.8760 - loss: 3.6010 - precision: 0.9003 - recall: 0.8511\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9012 - categorical_accuracy: 0.9012 - loss: 3.3999 - precision: 0.9135 - recall: 0.8890\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9435 - categorical_accuracy: 0.9435 - loss: 3.2240 - precision: 0.9564 - recall: 0.9260\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9430 - categorical_accuracy: 0.9430 - loss: 3.1258 - precision: 0.9510 - recall: 0.9253\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9478 - categorical_accuracy: 0.9478 - loss: 3.0163 - precision: 0.9557 - recall: 0.9336\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9503 - categorical_accuracy: 0.9503 - loss: 2.9204 - precision: 0.9595 - recall: 0.9447\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9659 - categorical_accuracy: 0.9659 - loss: 2.8189 - precision: 0.9658 - recall: 0.9548\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9434 - categorical_accuracy: 0.9434 - loss: 2.7802 - precision: 0.9541 - recall: 0.9375\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9679 - categorical_accuracy: 0.9679 - loss: 2.6912 - precision: 0.9689 - recall: 0.9596\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9301 - categorical_accuracy: 0.9301 - loss: 2.6919 - precision: 0.9389 - recall: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 341ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.0005  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.2179 - categorical_accuracy: 0.2179 - loss: 8.8547 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.4391 - categorical_accuracy: 0.4391 - loss: 7.9460 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5765 - categorical_accuracy: 0.5765 - loss: 7.1117 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6717 - categorical_accuracy: 0.6717 - loss: 6.2585 - precision: 0.8629 - recall: 0.1156\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6731 - categorical_accuracy: 0.6731 - loss: 5.5287 - precision: 0.8339 - recall: 0.3977\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7487 - categorical_accuracy: 0.7487 - loss: 4.9067 - precision: 0.8950 - recall: 0.6118\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8435 - categorical_accuracy: 0.8435 - loss: 4.4163 - precision: 0.9149 - recall: 0.7233\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8989 - categorical_accuracy: 0.8989 - loss: 4.0531 - precision: 0.9291 - recall: 0.8386\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9184 - categorical_accuracy: 0.9184 - loss: 3.7716 - precision: 0.9335 - recall: 0.9010\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9267 - categorical_accuracy: 0.9267 - loss: 3.5447 - precision: 0.9424 - recall: 0.9087\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9334 - categorical_accuracy: 0.9334 - loss: 3.3755 - precision: 0.9482 - recall: 0.9147\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9267 - categorical_accuracy: 0.9267 - loss: 3.2836 - precision: 0.9480 - recall: 0.9081\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9310 - categorical_accuracy: 0.9310 - loss: 3.1534 - precision: 0.9432 - recall: 0.9118\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9628 - categorical_accuracy: 0.9628 - loss: 2.9926 - precision: 0.9672 - recall: 0.9496\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9310 - categorical_accuracy: 0.9310 - loss: 2.9444 - precision: 0.9360 - recall: 0.9186\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9596 - categorical_accuracy: 0.9596 - loss: 2.8261 - precision: 0.9677 - recall: 0.9486\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9572 - categorical_accuracy: 0.9572 - loss: 2.7830 - precision: 0.9653 - recall: 0.9509\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9556 - categorical_accuracy: 0.9556 - loss: 2.7406 - precision: 0.9719 - recall: 0.9410\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9559 - categorical_accuracy: 0.9559 - loss: 2.7200 - precision: 0.9609 - recall: 0.9326\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9347 - categorical_accuracy: 0.9347 - loss: 2.6568 - precision: 0.9490 - recall: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (64, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step\n",
      "Iniciando o modelo com os seguintes parâmetros: Learning Rate -  0.001  Dropout -  0.2  LSTM Units -  256\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\gabriel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.3451 - categorical_accuracy: 0.3451 - loss: 7.9996 - precision: 0.2799 - recall: 0.0115   \n",
      "Epoch 2/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6840 - categorical_accuracy: 0.6840 - loss: 4.5957 - precision: 0.8117 - recall: 0.5843\n",
      "Epoch 3/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8451 - categorical_accuracy: 0.8451 - loss: 3.3289 - precision: 0.8740 - recall: 0.7866\n",
      "Epoch 4/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8615 - categorical_accuracy: 0.8615 - loss: 2.8299 - precision: 0.8882 - recall: 0.8395\n",
      "Epoch 5/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8893 - categorical_accuracy: 0.8893 - loss: 2.4658 - precision: 0.9091 - recall: 0.8802\n",
      "Epoch 6/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9388 - categorical_accuracy: 0.9388 - loss: 2.1429 - precision: 0.9517 - recall: 0.9315\n",
      "Epoch 7/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8971 - categorical_accuracy: 0.8971 - loss: 2.0420 - precision: 0.9209 - recall: 0.8862\n",
      "Epoch 8/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9458 - categorical_accuracy: 0.9458 - loss: 1.8004 - precision: 0.9513 - recall: 0.9363\n",
      "Epoch 9/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9468 - categorical_accuracy: 0.9468 - loss: 1.6567 - precision: 0.9622 - recall: 0.9361\n",
      "Epoch 10/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9582 - categorical_accuracy: 0.9582 - loss: 1.5249 - precision: 0.9676 - recall: 0.9512\n",
      "Epoch 11/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9521 - categorical_accuracy: 0.9521 - loss: 1.4221 - precision: 0.9565 - recall: 0.9460\n",
      "Epoch 12/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9568 - categorical_accuracy: 0.9568 - loss: 1.3083 - precision: 0.9633 - recall: 0.9519\n",
      "Epoch 13/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9613 - categorical_accuracy: 0.9613 - loss: 1.2174 - precision: 0.9686 - recall: 0.9573\n",
      "Epoch 14/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9600 - categorical_accuracy: 0.9600 - loss: 1.1436 - precision: 0.9652 - recall: 0.9515\n",
      "Epoch 15/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9455 - categorical_accuracy: 0.9455 - loss: 1.1484 - precision: 0.9507 - recall: 0.9390\n",
      "Epoch 16/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9642 - categorical_accuracy: 0.9642 - loss: 1.0463 - precision: 0.9682 - recall: 0.9592\n",
      "Epoch 17/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9733 - categorical_accuracy: 0.9733 - loss: 0.9529 - precision: 0.9800 - recall: 0.9681\n",
      "Epoch 18/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9472 - categorical_accuracy: 0.9472 - loss: 1.0136 - precision: 0.9540 - recall: 0.9412\n",
      "Epoch 19/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9624 - categorical_accuracy: 0.9624 - loss: 0.9163 - precision: 0.9671 - recall: 0.9567\n",
      "Epoch 20/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9481 - categorical_accuracy: 0.9481 - loss: 0.9219 - precision: 0.9562 - recall: 0.9369\n"
     ]
    }
   ],
   "source": [
    "params_grid = {\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20],\n",
    "    'model__learning_rate': [0.001, 0.0005],\n",
    "    'model__dropout_rate': [0.2],\n",
    "    'model__lstm_units': [128, 256]\n",
    "}\n",
    "\n",
    "# Criação do modelo\n",
    "model = KerasClassifier(build_fn=generate_model_grid_search, verbose=1)\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3  # 3-fold cross-validation\n",
    ")\n",
    "\n",
    "# Ajuste dos dados\n",
    "grid_result = grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor Acurácia: 0.7715\n",
      "Melhores Hiperparâmetros: {'batch_size': 32, 'epochs': 20, 'model__dropout_rate': 0.2, 'model__learning_rate': 0.001, 'model__lstm_units': 256}\n"
     ]
    }
   ],
   "source": [
    "print(\"Melhor Acurácia: {:.4f}\".format(grid_result.best_score_))\n",
    "print(\"Melhores Hiperparâmetros:\", grid_result.best_params_)\n",
    "\n",
    "# Melhor modelo\n",
    "best_model = grid_result.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,576</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ softmax[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                     │                   │            │ dot_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m8,576\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_4[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_5[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m394,240\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m65,792\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m65,792\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (\u001b[38;5;33mSoftmax\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_1 (\u001b[38;5;33mDot\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ softmax[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                     │                   │            │ dot_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,342,725</span> (5.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,342,725\u001b[0m (5.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,341,189</span> (5.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,341,189\u001b[0m (5.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_model():\n",
    "    # Dimensão das features (exemplo: 20 MFCCs + RMS + ZCR -> 22 features por timestep)\n",
    "    input_shape = (None, 22)  # None -> Sequência variável, 22 -> Features por timestep\n",
    "\n",
    "    # Regularização L2 (lambda = 0.01)\n",
    "    l2_reg = regularizers.L2(0.005)\n",
    "\n",
    "    # Entrada\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # 5 Blocos CNN\n",
    "    x = inputs\n",
    "    for _ in range(6):\n",
    "        x = layers.Conv1D(128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.2)(x)  # Dropout adicional\n",
    "        print(\"Após bloco CNN:\", x.shape)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    # LSTM com regularização L2\n",
    "    x = layers.LSTM(256, return_sequences=True, kernel_regularizer=l2_reg)(x)\n",
    "    print(\"Após LSTM (return_sequences=True):\", x.shape)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    # attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)  # 4 cabeças de atenção\n",
    "    # x = layers.Add()([x, attention])  # Resíduo para estabilizar treinamento\n",
    "    #print(\"Após Multi-Head Attention:\", x.shape)\n",
    "\n",
    "    # Atenção Personalizada\n",
    "    query = layers.Dense(256, activation=\"tanh\")(x)  # Projeta a sequência\n",
    "    key = layers.Dense(256, activation=\"tanh\")(x)    # Projeta a sequência\n",
    "    score = layers.Dot(axes=[2, 2])([query, key])    # Produto escalar\n",
    "    attention_weights = layers.Softmax()(score)     # Pesos normalizados\n",
    "    context = layers.Dot(axes=[2, 1])([attention_weights, x])  # Combina pesos com sequência\n",
    "\n",
    "    # Skip connection\n",
    "    x = layers.Add()([x, context])\n",
    "\n",
    "\n",
    "    # LSTM final para resumir a sequência\n",
    "    x = layers.LSTM(256, kernel_regularizer=l2_reg)(x)\n",
    "    print(\"Após LSTM final:\", x.shape)\n",
    "\n",
    "    # Camadas densas para classificação com regularização e Dropout\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=l2_reg)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    print(\"Após Dense:\", x.shape)\n",
    "\n",
    "    # Camada de saída multiclasse\n",
    "    num_classes = 5\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2_reg)(x)\n",
    "    print(\"Saída:\", outputs.shape)\n",
    "\n",
    "    # Construção do modelo\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    # Compilação\n",
    "    #optimizer = Adam(learning_rate=1e-3)  # Taxa de aprendizado inicial\n",
    "    # Learning Rate default do Adam = 0.001\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"categorical_accuracy\",  \"precision\", \"recall\"])\n",
    "    return model\n",
    "\n",
    "# Resumo do modelo\n",
    "model_cnn_rnn = generate_model()\n",
    "model_cnn_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: (1000, 95, 22), Teste: (250, 95, 22)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2  # Porcentagem para o conjunto de teste\n",
    "\n",
    "# Dividindo os dados em treino+validação e teste\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "print(f\"Treino: {X_train_val.shape}, Teste: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "k = 10  # Número de folds\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "train_accuracies = np.zeros((k, num_epochs))\n",
    "val_accuracies = np.zeros((k, num_epochs))\n",
    "\n",
    "# Configurando o K-Fold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista para armazenar as métricas de cada fold\n",
    "fold_losses = []\n",
    "fold_accuracies = []\n",
    "fold_categorical_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "histories = []\n",
    "\n",
    "# k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "    \n",
    "    # Dividindo os índices para treino e validação\n",
    "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"Primeiros valores de y_train:\", y_train[:5])\n",
    "    \n",
    "    # Criando o modelo\n",
    "    model = generate_model()\n",
    "    \n",
    "    # Early stopping para evitar overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    reduceOnPlateau = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "    # Configuração do ModelCheckpoint\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='/checkpoints/implementacao/feminino_doente_saudavel__multiclasse_melhor_modelo.keras',      # Caminho onde o modelo será salvo\n",
    "        monitor='val_loss',               # Métrica monitorada (ex: val_loss ou val_accuracy)\n",
    "        save_best_only=True,              # Salva apenas o melhor modelo\n",
    "        save_weights_only=False,          # Salva o modelo completo, incluindo arquitetura\n",
    "        mode='min',                       # Modo de monitoramento (min para loss, max para accuracy)\n",
    "        verbose=1                         # Exibe mensagens quando o modelo é salvo\n",
    "    )\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduceOnPlateau, checkpoint_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Salvar as acurácias\n",
    "    train_accuracies[fold] = history.history['accuracy']\n",
    "    val_accuracies[fold] = history.history['val_accuracy']\n",
    "    \n",
    "    # Avaliando no conjunto de validação\n",
    "    val_loss, val_accuracy, val_categorical_accuracy, val_precision, val_recall = model.evaluate(X_val, y_val, verbose=1)\n",
    "    print(f\"Validação - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_categorical_accuracies.append(val_categorical_accuracy)\n",
    "    fold_precisions.append(val_precision)\n",
    "    fold_recalls.append(val_recall)\n",
    "    #fold_f1_scores.append(val_f1_score)\n",
    "    histories.append(history)\n",
    "\n",
    "# Avaliação final no conjunto de teste\n",
    "print(\"\\nCross-validation completado.\")\n",
    "print(f\"Accuracies por fold: {fold_accuracies}\")\n",
    "print(f\"Accuracy média: {np.mean(fold_accuracies):.4f}, Desvio padrão: {np.std(fold_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7862 - categorical_accuracy: 0.7862 - loss: 1.3650 - precision: 0.8040 - recall: 0.7680  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3719, Accuracy: 0.7680, Categorical_accuracy Accuracy: 0.7680, Precision: 0.7949, Recall: 0.7440\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7870 - categorical_accuracy: 0.7870 - loss: 1.3181 - precision: 0.8194 - recall: 0.7792  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3404, Accuracy: 0.7720, Categorical_accuracy Accuracy: 0.7720, Precision: 0.7975, Recall: 0.7560\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7453 - categorical_accuracy: 0.7453 - loss: 1.3660 - precision: 0.7747 - recall: 0.7052  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3238, Accuracy: 0.7680, Categorical_accuracy Accuracy: 0.7680, Precision: 0.7913, Recall: 0.7280\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7790 - categorical_accuracy: 0.7790 - loss: 1.4193 - precision: 0.7882 - recall: 0.7386  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3968, Accuracy: 0.7800, Categorical_accuracy Accuracy: 0.7800, Precision: 0.7906, Recall: 0.7400\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8534 - categorical_accuracy: 0.8534 - loss: 1.1866 - precision: 0.8700 - recall: 0.8431  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.1868, Accuracy: 0.8520, Categorical_accuracy Accuracy: 0.8520, Precision: 0.8714, Recall: 0.8400\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8159 - categorical_accuracy: 0.8159 - loss: 1.2961 - precision: 0.8268 - recall: 0.7964  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3574, Accuracy: 0.7880, Categorical_accuracy Accuracy: 0.7880, Precision: 0.8042, Recall: 0.7720\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8166 - categorical_accuracy: 0.8166 - loss: 1.2987 - precision: 0.8345 - recall: 0.8011  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.3602, Accuracy: 0.8000, Categorical_accuracy Accuracy: 0.8000, Precision: 0.8107, Recall: 0.7880\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8159 - categorical_accuracy: 0.8159 - loss: 1.2912 - precision: 0.8475 - recall: 0.7841  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.2998, Accuracy: 0.8160, Categorical_accuracy Accuracy: 0.8160, Precision: 0.8390, Recall: 0.7920\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8303 - categorical_accuracy: 0.8303 - loss: 1.2704 - precision: 0.8386 - recall: 0.7957  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.2923, Accuracy: 0.8240, Categorical_accuracy Accuracy: 0.8240, Precision: 0.8326, Recall: 0.7960\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "Após bloco CNN: (None, None, 128)\n",
      "(None, None, 128)\n",
      "Após LSTM (return_sequences=True): (None, None, 256)\n",
      "Após LSTM final: (None, 256)\n",
      "Após Dense: (None, 128)\n",
      "Saída: (None, 5)\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7686 - categorical_accuracy: 0.7686 - loss: 1.3933 - precision: 0.7986 - recall: 0.7369  \n",
      "\n",
      "Desempenho no conjunto de teste - Loss: 1.4366, Accuracy: 0.7480, Categorical_accuracy Accuracy: 0.7480, Precision: 0.7879, Recall: 0.7280\n"
     ]
    }
   ],
   "source": [
    "# Avaliando o modelo no conjunto de teste\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_multiclass_accuracies = []\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "\n",
    "for i in range(10):\n",
    "    final_model = generate_model()\n",
    "    history_final = final_model.fit(X_train_val, y_train_val, epochs=num_epochs, batch_size=batch_size, verbose=0)\n",
    "    test_loss, test_accuracy, test_categorical_accuracy_accuracy, test_precision, test_recall = final_model.evaluate(X_test, y_test, verbose=1)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_multiclass_accuracies.append(test_categorical_accuracy_accuracy)\n",
    "    test_precisions.append(test_precision)\n",
    "    test_recalls.append(test_recall)\n",
    "    print(f\"\\nDesempenho no conjunto de teste - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Categorical_accuracy Accuracy: {test_categorical_accuracy_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Media test_losses \", np.mean(test_losses))\n",
    "print(\"Media test_accuracies \", np.mean(test_accuracies))\n",
    "print(\"Media test_binary_accuracies \", np.mean(test_multiclass_accuracies))\n",
    "print(\"Media test_precisions \", np.mean(test_precisions))\n",
    "print(\"Media test_recalls \", np.mean(test_recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(precisao, recall):\n",
    "    return 2 * (precisao * recall) / (precisao + recall)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x8ckNif9l6Y0",
    "f_vvAK5pUU38"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
